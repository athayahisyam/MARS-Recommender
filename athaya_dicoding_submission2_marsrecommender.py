# -*- coding: utf-8 -*-
"""Athaya-Dicoding-Submission2-MARSRecommender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10cCs_g8J88ABPe3aLMyDaE-yVNmahx6P

# Kelas Apa yang Harus Saya Ikuti?

Model Rekomendasi Kelas Daring

Dataset yang digunakan adalah dataset dari sistem pembelajaran [Office 365 MOOC](https://dileap.com/en/) yang diselenggarakan oleh Mandarine Academy, University of Lille. Data diterbitkan tahun 2022 dan disimpan pada repositori [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/BMY3UD).

Sitasi data:
Hafsa, M., Wattebled, P., Jacques, J., & Jourdan, L. (2023). E-learning recommender system dataset. Data in Brief, 47, 108942. https://doi.org/10.1016/j.dib.2023.108942

Sebagai catatan projek ini dikerjakan pada Google Colaboratory, semua *path* disesuaikan dengan spesifikasi pada Google Colaboratory.

## Pemerolehan Data

0. *Library Import* (ditambah sesuai kebutuhan)
"""

!pip install scikit-learn

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

"""1. Unggah data
2. Konversi data CSV ke dalam DataFrame
"""

# Explicit Rating
explir = pd.read_csv("/content/explicit_ratings_en.csv")

# Implicit Rating
implir = pd.read_csv("/content/implicit_ratings_en.csv")

# Items
items = pd.read_csv("/content/items_en.csv")

# Users
users = pd.read_csv("/content/users_en.csv")

print("Data unik pada explicit rating: ", len(explir.user_id.unique()))
print("Data unik pada implicit rating: ", len(implir.user_id.unique()))
print("Data unik pada users: ", len(users.user_id.unique()))
print("Data unik items: ", len(items.item_id.unique()))
print("------------------------------------")
print("Total data explicit rating: ", len(explir))
print("Total data implicit rating: ", len(implir))
print("Total data users: ", len(users))
print("Total data items: ", len(items))

"""Melalui proses pembacaan total data dan data unik pada masing-masing variabel dapat disimpulkan beberapa poin:

* Seluruh isi variabel `users` dan `items` adalah unik.
* Data pada *explicit rating* dan *implicit rating* memiliki selisih antara total data *explicit rating* dan *explicit rating* per pengguna unik, sebagaimana juga terjadi pada *implicit rating*.

## Analisis Data Univariat

### Data `Users`
"""

# Lima baris pertama
users.head()

# Tipe data
users.dtypes

# Job unik users
# Filter Job
users_filtered = users.dropna(subset=["job"])

# Tampilkan Job Unik Pengguna
unique_user_jobs = users_filtered["job"].unique()
print(unique_user_jobs)

# Cek Nilai kosong
print("Null Value pada Dataframe:")
print(users.isnull().sum())

# Bar Chart perbandingan antara pekerjaan dan null value
# Hitung jumlah user pada tiap pekerjaan
job_counts = users["job"].value_counts()

# Assign nama "empty" sebagai pengganti NaN pada user yang tidak memiliki data job
job_counts["empty"] = users["job"].isnull().sum()

# Plot bagan
plt.bar(job_counts.index, job_counts.values)

# Tambah Label
plt.xlabel("Kategori")
plt.ylabel("Jumlah Pengguna")
plt.title("Pengguna per Pekerjaan")

# Kustomisasi x-axis
plt.xticks(rotation=90)

# Tampilkan
plt.show()

# Rincian pekerjaan pengguna

print("Kategori pekerjaan dan jumlah user teridentifikasi:")
for job, count in job_counts.items():
    print(f"{job}: {count}")

"""Beberapa catatan yang dibuat dalam analisis ini:
1. Data pekerjaan (`job`) mayoritas tidak diisi
2. Data pekerjaan berbahasa Perancis, ditemukan beberapa redundansi makna

Karena mayoritas data null dan penelitian tidak mendukung penggunaan bahasa Perancis, fitur `job` akan di *drop*

### Data `Items`
"""

items.head()

items.info()

# Create a copy of the original "items" DataFrame for preprocessing
items_processed = items.copy()

# Replace [] with the string "empty" in the "Job" column
items_processed["Job"] = items_processed["Job"].apply(lambda x: "empty" if x == '[]' else x)

# Function to remove square brackets and apostrophe signs
def remove_square_brackets_and_apostrophes(val):
    if isinstance(val, str):
        # Remove square brackets
        val = val.replace("[", "").replace("]", "")
        # Remove apostrophes
        val = val.replace("'", "")
    return val

# Apply the remove_square_brackets_and_apostrophes function to the "Job" column
items_processed["Job"] = items_processed["Job"].apply(remove_square_brackets_and_apostrophes)

# items_processed info
items_processed.info()

print("----------------------------------")

# Check for missing values again after handling the lists and multiple job values
print("Missing Values in Items Processed DataFrame:")
print(items_processed.isnull().sum())

# Bahasa yang digunakan item
print("Bahasa-bahasa unik pada tiap item:")
print(items["language"].unique())

print("------------------------------")

# Tingkat kesulitan
print("Tingkat kesulitan unik:")
print(items["Difficulty"].unique())

print("------------------------------")

# Jenis kursus
print("Tipe kursus unik:")
print(items["type"].unique())

print("------------------------------")

# Distribusi durasi kursus
print("Statistik durasi kursus")
print(items["duration"].describe())

print("------------------------------")

# Cek Nilai kosong
print("Null Value pada Dataframe:")
print(items.isnull().sum())

items_processed.head()

"""Beberapa catatan yang dibuat dalam analisis ini:

1. Seluruh *course* menggunakan bahasa Inggris (`en`)
2. Terdapat tiga tipe tingkat kesulitan (`Beginner`, `Intermediate`, `Advanced`) dan beberapa tidak memiliki definisi. Ini akan diatasi dengan menjadikan item yang tidak memiliki tingkat kesulitan menjadi `general`.
3. Terdapat tiga jenis *course*: `tutorial`, `webcast` dan `use_case`
4. Pada dataset ini, `job` memiliki nilai kosong, akan tetapi diisi dengan kurung siku `[]`, sehingga butuh dibersihkan.
5. Rata-rata durasi video 179 second (2 - 3 menit)
6. Beberapa video tidak memiliki *view*
7. Beberapa deskripsi kosong.

### Data `Explir` atau *Explicit Rating*
"""

explir.head()

explir.info()

# Rata-rata Watch Percentage
print("Statistik Watch Percentage:")
print(explir["watch_percentage"].describe())

print("-------------------------")

# Cek Null Value
print("Null Value:")
print(explir.isnull().sum())

"""Beberapa catatan yang dibuat dalam analisis ini:

1. Pada data *explicit rating* tidak ada data kosong
2. Rata-rata presentasi menonton: 84%

### Data `Implir` atau *Implicit Rating*
"""

implir.head()

implir.info()

# Cek Nilai Kosong
print("Nilai Kosong pada Data Frame:")
print(implir.isnull().sum())

"""Dari analisis yang dilakukan, tidak ditemukan adanya permasalahan pada data `implir` atau *Implicit Rating*.

## Data Preprocessing

### Pembersihan Data dari Dataset `Items`
"""

# Buat variabel baru
items_processed = items.copy()

items_processed.info()

# Kolom Jobs
# Ganti kurung siku kosong dengan "empty"
items_processed["Job"] = items_processed["Job"].apply(lambda x: "empty" if x == '[]' else x)

# Fungsi menghapus tanda petik dan kurung siku
def remove_square_brackets_and_apostrophes(val):
    if isinstance(val, str):
        # Hapus kurung siku
        val = val.replace("[", "").replace("]", "")
        # Hapus petik
        val = val.replace("'", "")
    return val

items_processed["Job"] = items_processed["Job"].apply(remove_square_brackets_and_apostrophes)

# Kolom Software
# Hapus kurung siku dan petik atas pada software
def remove_square_brackets_and_apostrophes_from_software(val):
    if isinstance(val, str):
        # Hapus kurung siku
        val = val.replace("[", "").replace("]", "")
        # Hapus petik atas
        val = val.replace("'", "")
    return val

items_processed["Software"] = items_processed["Software"].apply(remove_square_brackets_and_apostrophes_from_software)

# Tampilkan entry dengan data lebih dari 1
software_with_multiple_entries = items_processed[items_processed["Software"].str.contains(",")]
print(software_with_multiple_entries["Software"])

# Kolom Theme
# Fungsi menghapus petik dan kurung siku
def remove_square_brackets_and_apostrophes_from_theme(val):
    if isinstance(val, str):
        # Hapus kurung siku
        val = val.replace("[", "").replace("]", "")
        # Hapus petik
        val = val.replace("'", "")
    return val

items_processed["Theme"] = items_processed["Theme"].apply(remove_square_brackets_and_apostrophes_from_theme)

items_processed.head()

# Cek Nilai Kosong
print("Nilai Kosong pada Data Frame:")
print(items_processed.isnull().sum())

items_processed.info()

"""### *Resolving Null Values*

#### `Nb_views`

Untuk mencegah bias yang akan terjadi bila nilai kosong pada `Items` di ubah menjadi 0, maka digunakan metode Imputasi menggunakan rata-rata.
"""

# Perhitungan rata-rata data non-null pada nb_views
mean_nb_views = items_processed["nb_views"].mean()

# Menggantikan data null dengan rata-rata
items_processed["nb_views"].fillna(mean_nb_views, inplace=True)

# Periksa null_value
print("Null Value pada DataFrame:")
print(items_processed.isnull().sum())

items_processed.info()

"""#### `Difficulty`

Karena *Null Value* pada data `difficulty` cukup besar (692 dari 1167 data), dan kolom tersebut merupakan data kategorikal, digunakan Imputasi dengan Model Machine Learning, untuk menentukan tingkat kesulitan pada data yang kosong berdasarkan fitur-fitur yang ada. Pada kasus ini, algoritma yang cocok digunakan adalah algoritma klasifikasi, yaitu, Algoritma Random Forest.
"""

# Membuat salinan variable items
items_processed_imputed = items_processed.copy()

# One Hot Encoding pada data kategorikal
encoder = OneHotEncoder(drop='first', sparse=False)
type_encoded = encoder.fit_transform(items_processed_imputed[['type']])
type_encoded_df = pd.DataFrame(type_encoded, columns=[f'type_{i}' for i in range(type_encoded.shape[1])])

# Drop kolom type
items_processed_imputed.drop('type', axis=1, inplace=True)
items_processed_imputed = pd.concat([items_processed_imputed, type_encoded_df], axis=1)

# Data Preparation dengan y_train berisi data Difficulty yang Null
X_train = items_processed_imputed.loc[items_processed_imputed['Difficulty'].notnull(), ['nb_views', 'duration'] + type_encoded_df.columns.tolist()]
y_train = items_processed_imputed.loc[items_processed_imputed['Difficulty'].notnull(), 'Difficulty']

X_test = items_processed_imputed.loc[items_processed_imputed['Difficulty'].isnull(), ['nb_views', 'duration'] + type_encoded_df.columns.tolist()]

# Latih Model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Prediksi Null Value
predicted_difficulty = model.predict(X_test)

# Perbarui data pada items_processed_imputed
items_processed_imputed.loc[items_processed_imputed['Difficulty'].isnull(), 'Difficulty'] = predicted_difficulty

print(items_processed_imputed.isnull().sum())

items_processed_imputed.info()

# Membuat diagram batang untuk perbandingan jumlah kursus per tingkat kesulitan
# Hitung berapa kali kemunculan tiap level kesulitan pada data lama
old_difficulty_counts = items_processed['Difficulty'].value_counts()

# Hitung berapa kali kemunculan tiap level kesulitan pada data baru
new_difficulty_counts = items_processed_imputed['Difficulty'].value_counts()

# Penggabungan
combined_counts = pd.concat([old_difficulty_counts, new_difficulty_counts], axis=1)
combined_counts.columns = ['Original Data', 'Imputed Data']

# Plotting diagram
combined_counts.plot(kind='bar')
plt.xlabel('Level Difficulty')
plt.ylabel('Jumlah')
plt.title('Distribusi Tingkat Kesulitan dibanding data lama dan baru')
plt.show()

# Pembaruan data pada variabel items_processed
items_processed.update(items_processed_imputed['Difficulty'])

# Cek
print(items_processed.isnull().sum())

items_processed.info()

"""Karena sistem rekomendasi tidak menggunakan data Deskripsi yang berisi teks dan memiliki null values yang besar dan tidak bisa diisi dengan model secara sederhana, data deskripsi diabaikan.

Data *Implicit Ratings* dan *Explicit Rating* tidak memiliki Null Values, maka tidak melalui *data preprocessing*.

## Data Preparation

Untuk memudahkan proses *encoding* dan pelatihan model dilakukan penggabungan data.

### Data Merging
"""

# Buat dataframe baru: prepared_cbf
prepared_cbf = pd.DataFrame()

# Masukan user_id, item_id dan rating dari explir
prepared_cbf['user_id'] = explir['user_id']
prepared_cbf['item_id'] = explir['item_id']
prepared_cbf['rating'] = explir['rating']

# Masukan data dari items_processed
item_attributes = items_processed[['item_id', 'duration', 'Difficulty', 'type']]
prepared_cbf = pd.merge(prepared_cbf, item_attributes, on='item_id', how='left')

prepared_cbf.head()

prepared_cbf.info()

print(prepared_cbf.isnull().sum())

"""### *One-Hot Encoding* data CBF

*One hot encoding* berfungsi mengubah data kategorikal dan teks menjadi data numerik. Metode ini dipilih untuk mengubah data pada kolom `type` dan `Difficulty` menjadi data numerik agar dapat diproses oleh model *machine learning*. Data `Difficulty` dan `type` terdiri dari tiga kategori. Pada `Difficulty` terdiri dari `Beginner`, `Intermediate` dan `Advanced`. Data `type` terdiri dari `webcast`, `tutorial`, dan `use_case`. Karena jumlah kategori sedikit, metode *one hot encoding*.
"""

# Encoding Type
one_hot_type = pd.get_dummies(prepared_cbf['type'], prefix='type')

# Encoding Difficulty
one_hot_difficulty = pd.get_dummies(prepared_cbf['Difficulty'], prefix='Difficulty')

# Pemindahan dengan melalukan Concatenate
prepared_cbf_encoded = pd.concat([prepared_cbf, one_hot_type, one_hot_difficulty], axis=1)

# Drop kolom type dan difficulty original
prepared_cbf_encoded.drop(['type', 'Difficulty'], axis=1, inplace=True)

prepared_cbf_encoded.head()

prepared_cbf_encoded.info()

"""### *One-Hot Encoding* Data *Course Features* dan Feature Scaling

Untuk membuat *course embbeding* dibutuhkan *one_hot encoding* pada data `course_features`.
"""

# Pilih kolom relevan
course_features = items_processed[['item_id', 'duration', 'Difficulty', 'type']]

# One-hot encoding untuk kolom  'Difficulty' dan 'type'
course_features_encoded = pd.get_dummies(course_features, columns=['Difficulty', 'type'], drop_first=False)

# Feature scaling dengan normalisai
scaler = MinMaxScaler()
numerical_features = ['duration', 'Difficulty_Advanced', 'Difficulty_Beginner', 'Difficulty_Intermediate',
                      'type_tutorial', 'type_use_case', 'type_webcast']

course_features_encoded[numerical_features] = scaler.fit_transform(course_features_encoded[numerical_features])

course_features_encoded.head()

"""### Membuat `User Profile`"""

# Grup data user
user_profiles = prepared_cbf_encoded.groupby('user_id').mean().reset_index()

# Menghilangkan kolom yang tidak dipakai
user_profiles = user_profiles.drop(['item_id', 'rating'], axis=1)

user_profiles.head()

user_profiles.tail()

user_profiles.info()

items_processed.info()

"""## Pemodelan *Content Based Filtering*

Pada tahap ini dilakukan filter berbasis konten.

### Pengukuran Similaritas
"""

# Embedding pada user_profile
user_profiles_array = user_profiles.drop('user_id', axis=1).values

# Embedding pada course_features
course_embeddings_array = course_features_encoded.drop('item_id', axis=1).values

# Cosine Similarity antara course_embedding dan user_profile embedding
similarity_scores = cosine_similarity(user_profiles_array, course_embeddings_array)

"""### Pembuatan Rekomendasi"""

# Fungsi rekomendasi
def generate_user_recommendations(user_id, num_recommendations):
    # Karena user_id di drop, mencari padanan index pada row user_id dan user_profile
    try:
        user_row_index = user_profiles[user_profiles['user_id'] == user_id].index[0]
    except IndexError:
        print("Invalid user_id. User ID not found.")
        return None

    # Get similarity score
    user_similarity_scores = similarity_scores[user_row_index]

    # Top N course sesuai permintaan
    top_course_indices = np.argsort(user_similarity_scores)[::-1][:num_recommendations]

    # Pengambilan id untuk course yang terdapat pada rekomendasi
    top_course_ids = course_features_encoded.iloc[top_course_indices]['item_id']

    # Map id pada nama kursus
    recommended_courses = items_processed[items_processed['item_id'].isin(top_course_ids)]['name']

    return recommended_courses

# masukkan variabel id disini, cek pada user_profile untuk user_id yang tersedia
# masukkan jumlah rekomendasi

user_id = 608559
num_recommendations = 10
user_recommendations = generate_user_recommendations(user_id, num_recommendations)
print(user_recommendations)